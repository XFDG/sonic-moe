import argparse
import random
import time
from typing import Tuple, Type

import cutlass
import torch
import torch.nn.functional as F
from rich import print as print0
from triton.testing import do_bench # ç”¨äºç²¾ç¡®æµ‹é‡ GPU æ—¶é—´çš„å·¥å…·

from sonicmoe import MoE
from sonicmoe.enums import ActivationType, is_glu
from sonicmoe.functional import moe_TC_softmax_topk_layer

# --- ã€å¯å­¦ä¹ ç‚¹ã€‘è¿™äº›å‡½æ•°å®šä¹‰äº† MoE ä¸“å®¶å†…éƒ¨ä½¿ç”¨çš„å„ç§æ¿€æ´»å‡½æ•° ---
def swiglu(x: torch.Tensor) -> torch.Tensor:
    u = x[..., 1::2]
    g = x[..., ::2]
    return u * F.silu(g)

# ... (å…¶ä»–æ¿€æ´»å‡½æ•°çœç•¥ï¼Œé€»è¾‘ä¸€è‡´)

def parse_arguments() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="SonicMoE æ€§èƒ½ç ”ç©¶è„šæœ¬")

    # --- ã€å¯ä¿®æ”¹é¡¹ 1ã€‘æ ¸å¿ƒç»´åº¦å‚æ•° ---
    # é»˜è®¤å€¼: T=32768 (Tokens), H=4096 (Hidden), I=1024 (Inter), E=128 (Experts), K=8 (Top-k)
    parser.add_argument(
        "--thiek",
        type=lambda s: tuple([int(x.strip()) for x in s.split(",")]),
        #default=(32768, 4096, 1024, 128, 8),
        default=(32768*2, 4096, 1024, 128, 8), #A
        #default=(32768, 4096, 1024, 128, 8), #B
        help="ä¿®æ”¹è¿™ 5 ä¸ªå€¼å¯ä»¥æµ‹è¯•ä¸åŒè´Ÿè½½ä¸‹çš„ H100 è¡¨ç°",
    )

    # --- ã€å¯ä¿®æ”¹é¡¹ 2ã€‘æ¿€æ´»å‡½æ•°é€‰æ‹© ---
    parser.add_argument(
        "--activation", choices=["swiglu", "geglu", "relu", "silu"], default="swiglu"
    )

    # --- ã€å¯ä¿®æ”¹é¡¹ 3ã€‘æ˜¯å¦è·³è¿‡ç²¾åº¦æ ¡éªŒ ---
    # å¦‚æœä½ åªæƒ³çœ‹é€Ÿåº¦ï¼Œå¼€å¯æ­¤é¡¹å¯ä»¥èŠ‚çœå¤§çº¦ 1 åˆ†é’Ÿçš„ CPU æ¯”å¯¹æ—¶é—´
    parser.add_argument("--skip_test", action="store_true", default=False)

    return parser.parse_args()

def run(thiek, activation, skip_test):
    T, H, I, E, K = thiek
    print(f"ğŸš€ å¼€å§‹æµ‹è¯•: Tokens={T}, ä¸“å®¶æ€»æ•°={E}, æ¯æ¬¡æ¿€æ´»ä¸“å®¶={K}, æ¿€æ´»å‡½æ•°={activation}")

    # --- åˆå§‹åŒ– SonicMoE å±‚ ---
    # è¿™é‡Œä¼šç”³è¯·æ˜¾å­˜å¹¶åˆå§‹åŒ–æƒé‡
    moe = MoE(
        num_experts=E,
        num_experts_per_tok=K,
        hidden_size=H,
        intermediate_size=I,
        activation_function=ActivationType(activation),
    ).to(dtype=torch.bfloat16).cuda() # H100 æ¨èä½¿ç”¨ bfloat16

    x = 0.2 * torch.randn(T, H, device="cuda", dtype=torch.bfloat16, requires_grad=True)
    w1, w2, router_w = moe.c_fc.weight, moe.c_proj.weight, moe.router.weight
    dout = 0.2 * torch.randn_like(x)

    # --- ã€æ ¸å¿ƒé€»è¾‘ã€‘ç²¾åº¦æ ¡éªŒ (Reference Check) ---
    if not skip_test:
        print("æ­£åœ¨è¿›è¡Œæ•°å­¦ç²¾åº¦æ ¡éªŒ (ä¸æ ‡å‡† PyTorch ç»“æœå¯¹æ¯”)...")
        # è°ƒç”¨ SonicMoE çš„åŠ é€Ÿå†…æ ¸
        o, _, _ = moe_TC_softmax_topk_layer(x, router_w, w1.permute(1, 2, 0), None, w2.permute(1, 2, 0), None, K, 0, ActivationType(activation))
        # ... (æ­¤å¤„çœç•¥æ¯”å¯¹é€»è¾‘ï¼ŒæˆåŠŸåˆ™æ‰“å° PASS)

    # --- ã€æ€§èƒ½æµ‹é‡ã€‘è®¡ç®—é‡ (FLOPs) ç»Ÿè®¡ ---
    # å¯¹äº SwiGLUï¼Œè®¡ç®—é‡å…¬å¼ä¸º 6 * T * I * H * K
    flops_fwd = 6 * T * I * H * K 
    
    # --- ã€å¯ä¿®æ”¹é¡¹ 4ã€‘æµ‹è¯•å¾ªç¯æ¬¡æ•° ---
    repeats = 500 # å¢åŠ æ¬¡æ•°å¯ä»¥è·å¾—æ›´ç¨³å¥çš„å¹³å‡å€¼
    warmup = 10   # é¢„çƒ­æ¬¡æ•°ï¼Œç¡®ä¿ GPU é¢‘ç‡ç¨³å®š

    # æµ‹é‡æ¨ç†æ€§èƒ½ (Inference)
    fwd_timing = do_bench(lambda: moe(x)[0], warmup=warmup, rep=repeats)
    tflops = flops_fwd / (fwd_timing * 1e9)
    print0(f"[bold green]âœ… æ¨ç†æ€§èƒ½: {fwd_timing:.3f} ms, TFLOPS: {tflops:.1f}[/bold green]")

if __name__ == "__main__":
    args = parse_arguments()
    run(args.thiek, args.activation, args.skip_test)