# ********************************************************************************
# Copyright (c) 2025, Wentao Guo, Mayank Mishra, Xinle Cheng, Ion Stoica, Tri Dao
# ********************************************************************************

import argparse
import random
import time
from typing import Tuple, Type

import cutlass
import torch
import torch.nn.functional as F
from rich import print as print0
from triton.testing import do_bench

from sonicmoe import MoE
from sonicmoe.enums import ActivationType, is_glu
from sonicmoe.functional import moe_TC_softmax_topk_layer

# --- æ¿€æ´»å‡½æ•°å®šä¹‰åŒºåŸŸ (æ— éœ€ä¿®æ”¹) ---
def swiglu(x: torch.Tensor) -> torch.Tensor:
    u = x[..., 1::2]
    g = x[..., ::2]
    return u * F.silu(g)

def geglu(x: torch.Tensor) -> torch.Tensor:
    u = x[..., 1::2]
    g = x[..., ::2]
    return F.gelu(g.float()).to(dtype=g.dtype) * u

def gelu(x: torch.Tensor) -> torch.Tensor:
    return F.gelu(x.float()).to(dtype=x.dtype)

def reglu(x: torch.Tensor) -> torch.Tensor:
    u = x[..., 1::2]
    g = x[..., ::2]
    return (F.relu(g.float()) * u).to(dtype=g.dtype)

def relu(x: torch.Tensor) -> torch.Tensor:
    return F.relu(x)

def relu_sq(x: torch.Tensor) -> torch.Tensor:
    return F.relu(x) ** 2

def silu(x: torch.Tensor) -> torch.Tensor:
    return F.silu(x)

# --- å‚æ•°è§£æè¾…åŠ©å‡½æ•° ---
def parse_comma_separated_ints(s: str):
    try:
        return tuple([int(x.strip()) for x in s.split(",")])
    except ValueError:
        raise argparse.ArgumentTypeError("Invalid format. Expected comma-separated integers.")

def parse_arguments() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="SonicMoE æ€§èƒ½æµ‹è¯•ä¸æŠ¥å‘Šç”Ÿæˆè„šæœ¬")

    # ================= [æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ] =================
    # --thiek å‚æ•°å®šä¹‰äº†æ¨¡å‹çš„è§„æ¨¡ã€‚
    # T (Tokens): ä¸€æ¬¡å¤„ç†çš„Tokenæ•°é‡ (å¦‚ 32768)
    # H (Hidden): æ¨¡å‹éšè—å±‚ç»´åº¦ (å¦‚ 4096)
    # I (Intermediate): ä¸“å®¶å†…éƒ¨ç»´åº¦ (å¦‚ 1024)
    # E (Experts): ä¸“å®¶æ€»æ•° (å¦‚ 128) - ä¿®æ”¹è¿™é‡Œå¯ä»¥æµ‹è¯•æ˜¾å­˜å‹åŠ›
    # K (Top-K): æ¯ä¸ªTokené€‰å‡ ä¸ªä¸“å®¶ (å¦‚ 8)
    parser.add_argument(
        "--thiek",
        type=parse_comma_separated_ints,
        default=(32768, 4096, 1024, 128, 8), # <--- å¦‚æœä¸ä¼ å‚ï¼Œé»˜è®¤è·‘è¿™ä¸ªé…ç½®
        help="æ ¼å¼: T,H,I,E,K (ä¾‹å¦‚: 32768,4096,1024,128,8)",
    )
    # =================================================

    parser.add_argument(
        "--dtype",
        type=cutlass.dtype,
        default=cutlass.BFloat16, # H100 é»˜è®¤ä½¿ç”¨ BFloat16 ä»¥è·å¾—æœ€ä½³æ€§èƒ½
    )
    parser.add_argument(
        "--skip_test",
        action="store_true",
        default=False, # å¦‚æœè®¾ä¸º Trueï¼Œå°†è·³è¿‡æ•°å­¦ç²¾åº¦æ£€æŸ¥ï¼Œç›´æ¥è·‘åˆ†
    )
    parser.add_argument(
        "--activation", 
        choices=["swiglu", "geglu", "reglu", "relu_sq", "relu", "silu", "gelu"], 
        default="swiglu" # Llama ç­‰ä¸»æµæ¨¡å‹é€šå¸¸ä½¿ç”¨ SwiGLU
    )
    parser.add_argument(
        "--add_bias",
        action="store_true",
        default=False,
    )
    args = parser.parse_args()

    if len(args.thiek) != 5:
        parser.error("--thiek must contain exactly 5 values")

    return args


def run(
    thiek: Tuple[int, int, int, int, int],
    dtype: Type[cutlass.Numeric],
    skip_test: Type[bool],
    add_bias: Type[bool],
    activation: Type[str],
    **kwargs,
):
    # æ ¹æ®å‚æ•°é€‰æ‹© PyTorch æ•°æ®ç±»å‹
    torch_dtype = {cutlass.BFloat16: torch.bfloat16, cutlass.Float16: torch.float16}[dtype]
    activation = ActivationType(activation)
    
    # è§£åŒ…ç»´åº¦å‚æ•°
    T, H, I, E, K = thiek
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯•é…ç½®: Tokens(T)={T}, Hidden(H)={H}, Intermediate(I)={I}, Experts(E)={E}, Top-K(K)={K}")

    # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡è·‘çš„ç»“æœä¸€è‡´
    random.seed(1111)
    torch.manual_seed(1111)
    torch.cuda.manual_seed_all(1111)

    # --- [åˆå§‹åŒ– MoE å±‚] ---
    # è¿™é‡Œä¼šç”³è¯· H100 æ˜¾å­˜ã€‚å¦‚æœ E (ä¸“å®¶æ•°) å¤ªå¤§ï¼Œè¿™é‡Œå¯èƒ½ä¼šçˆ†æ˜¾å­˜ã€‚
    try:
        moe = (
            MoE(
                num_experts=E,
                num_experts_per_tok=K,
                hidden_size=H,
                intermediate_size=I,
                activation_function=activation,
                add_bias=add_bias, # å¿…é¡»ä¼ å…¥ï¼Œå¦åˆ™ä¼šæŠ¥é”™
                std=0.02,          # å¿…é¡»ä¼ å…¥ï¼Œå¦åˆ™ä¼šæŠ¥é”™
            )
            .to(dtype=torch_dtype)
            .cuda()
        )
    except torch.cuda.OutOfMemoryError:
        print("âŒ åˆå§‹åŒ–å¤±è´¥ï¼šæ˜¾å­˜ä¸è¶³ (CUDA OOM)ã€‚è¯·å°è¯•å‡å°ä¸“å®¶æ•°é‡ E æˆ– Token æ•°é‡ Tã€‚")
        return

    # ç”Ÿæˆéšæœºè¾“å…¥æ•°æ®
    x = 0.2 * torch.randn(T, H, device="cuda:0", dtype=torch_dtype, requires_grad=True)
    w1, w2, router_w = moe.c_fc.weight, moe.c_proj.weight, moe.router.weight
    b1, b2 = moe.c_fc.bias, moe.c_proj.bias
    
    # åˆå§‹åŒ– Bias
    if add_bias:
        torch.nn.init.normal_(b1, 0, 0.01)
        torch.nn.init.normal_(b2, 0, 0.01)
    
    dout = 0.2 * torch.randn_like(x, requires_grad=True)

    # --- [ç¬¬ä¸€é˜¶æ®µï¼šæ•°å­¦æ­£ç¡®æ€§æ£€æŸ¥] ---
    if not skip_test:
        print("ğŸ” æ­£åœ¨è¿›è¡Œæ•°å­¦ç²¾åº¦æ ¡éªŒ (å¯¹æ¯”æ ‡å‡† PyTorch å®ç°)...")
        # è¿™é‡Œçš„ moe_TC_softmax_topk_layer æ˜¯ SonicMoE çš„æ ¸å¿ƒåŠ é€Ÿç®—å­
        o, router_logits, expert_frequency = moe_TC_softmax_topk_layer(
            x, router_w, w1.permute(1, 2, 0), b1, w2.permute(1, 2, 0), b2, moe.top_k, moe.stream_id, activation
        )
        
        # è®¡ç®— SonicMoE çš„æ¢¯åº¦
        if add_bias:
            dx, dw1, db1, dw2, db2, drouter_w = torch.autograd.grad(
                o, [x, w1, b1, w2, b2, router_w], grad_outputs=dout
            )
        else:
            dx, dw1, dw2, drouter_w = torch.autograd.grad(o, [x, w1, w2, router_w], grad_outputs=dout)

        # --- ä½¿ç”¨æ ‡å‡† PyTorch æ‰‹å†™ä¸€ä¸ª MoE è¿›è¡Œå¯¹æ¯” ---
        logits = F.linear(x, router_w)
        ref_topk_logits, ref_topk_experts = logits.topk(K, dim=-1)
        ref_topk_scores = ref_topk_logits.softmax(dim=-1, dtype=torch.float32)

        # ... (ä¸­é—´çœç•¥äº†ç¹ççš„ PyTorch å‚è€ƒå®ç°é€»è¾‘) ...
        # æ ¸å¿ƒé€»è¾‘ï¼šç”¨ for å¾ªç¯æ¨¡æ‹Ÿä¸“å®¶è®¡ç®—ï¼Œä½œä¸ºâ€œæ ‡å‡†ç­”æ¡ˆâ€

        act_func = {
            ActivationType.SWIGLU: swiglu,
            ActivationType.GEGLU: geglu,
            ActivationType.REGLU: reglu,
            ActivationType.GELU: gelu,
            ActivationType.RELU: relu,
            ActivationType.SILU: silu,
            ActivationType.RELU_SQ: relu_sq,
        }[activation]

        # éªŒè¯å‰å‘ä¼ æ’­ç»“æœ
        with torch.autocast("cuda:0", torch.float32):
            ref_o = torch.zeros_like(x)
            for i in range(E):
                # æ‰¾åˆ°åˆ†é…ç»™ç¬¬ i ä¸ªä¸“å®¶çš„ token
                T_idx, E_idx = torch.argwhere(ref_topk_experts == i).split(1, dim=1)
                T_idx, E_idx = T_idx.squeeze(-1), E_idx.squeeze(-1)

                if T_idx.numel() > 0:
                    w1_out = F.linear(x[T_idx, :], w1[i, :, :].squeeze(), bias=(b1[i] if add_bias else None))
                    w1_out = act_func(w1_out)
                    w2_out = F.linear(w1_out, w2[i, :, :].squeeze(), bias=(b2[i] if add_bias else None))
                    ref_o[T_idx, :] += w2_out * ref_topk_scores[T_idx, E_idx, None]

            # æ‰“å°è¯¯å·®
            o_diff = (o.float() - ref_o).abs()
            print(f"   æœ€å¤§ç›¸å¯¹è¯¯å·® (Mean Rel Diff): {(o_diff / (ref_o.abs() + 1e-6)).mean():.6f}")

            # éªŒè¯åå‘ä¼ æ’­æ¢¯åº¦
            if add_bias:
                 ref_dx, ref_dw1, ref_db1, ref_dw2, ref_db2, ref_drouter_w = torch.autograd.grad(
                    ref_o, [x, w1, b1, w2, b2, router_w], grad_outputs=dout
                )
            else:
                ref_dx, ref_dw1, ref_dw2, ref_drouter_w = torch.autograd.grad(
                    ref_o, [x, w1, w2, router_w], grad_outputs=dout
                )
            
            # ç®€å•çš„æ¢¯åº¦æ£€æŸ¥æ‰“å°
            print(f"   æ¢¯åº¦æ£€æŸ¥ (drouter_w) ç›¸å¯¹è¯¯å·®: {((drouter_w - ref_drouter_w).abs() / (ref_drouter_w.abs() + 1e-6)).mean():.6f}")
    
    # --- [ç¬¬äºŒé˜¶æ®µï¼šæ€§èƒ½è·‘åˆ† Benchmarking] ---
    print("\nâ±ï¸  æ­£åœ¨è¿›è¡Œæ€§èƒ½æµ‹è¯• (Warmup + Benchmark)...")
    
    # è®¡ç®—ç†è®º FLOPs (æµ®ç‚¹è¿ç®—æ¬¡æ•°)
    if is_glu(activation):
        flops = 6 * T * I * H * K
    else:
        flops = 4 * T * I * H * K

    repeats = 500 # é‡å¤è·‘ 500 æ¬¡å–å¹³å‡
    warmup = 5    # é¢„çƒ­ 5 æ¬¡

    time.sleep(0.5)

    # 1. ç¼–è¯‘æ¨¡å¼ (Torch Compile) æµ‹è¯•
    @torch.compile
    def forward_only(is_inference_mode_enabled):
        o, _, _ = moe_TC_softmax_topk_layer(
            x, router_w, w1.permute(1, 2, 0), b1, w2.permute(1, 2, 0), b2, moe.top_k, moe.stream_id, activation, is_inference_mode_enabled
        )
        return o

    # æµ‹è¯• A: æ™®é€šå‰å‘ (Fwd)
    fwd_timing = do_bench(lambda: forward_only(False), warmup=warmup, rep=repeats)
    tflops = flops / (fwd_timing * 1e9)
    print0(f"[bold green]   [Mode: Training Fwd] Average time: {fwd_timing:.3f} ms, TFLOPS: {tflops:.1f}[/bold green]")

    time.sleep(0.5)

    # æµ‹è¯• B: æ¨ç†æ¨¡å¼ (Inference Mode) - é€šå¸¸æœ€å¿«
    timing = do_bench(lambda: forward_only(True), warmup=warmup, rep=repeats)
    tflops_inf = flops / (timing * 1e9)
    print0(f"[bold green]   [Mode: Inference   ] Average time: {timing:.3f} ms, TFLOPS: {tflops_inf:.1f}[/bold green]")

    # æµ‹è¯• C: å®Œæ•´è®­ç»ƒ (Fwd + Bwd)
    @torch.compile
    def forward_and_backward():
        o, _, _ = moe_TC_softmax_topk_layer(
            x, router_w, w1.permute(1, 2, 0), b1, w2.permute(1, 2, 0), b2, moe.top_k, moe.stream_id, activation, False
        )
        o.backward(dout, retain_graph=True)
        # æ¸…ç©ºæ¢¯åº¦ä»¥ä¾¿ä¸‹ä¸€æ¬¡å¾ªç¯
        x.grad = w1.grad = w2.grad = router_w.grad = None

    if is_glu(activation):
        flops_bwd = 18 * T * I * H * K
    else:
        flops_bwd = 12 * T * I * H * K

    e2e_timing = do_bench(forward_and_backward, warmup=warmup, rep=repeats, grad_to_none=[x, w1, w2, router_w, dout])
    tflops_e2e = flops_bwd / (e2e_timing * 1e9)
    print0(f"[bold green]   [Mode: Train Full  ] Average time: {e2e_timing:.3f} ms, TFLOPS: {tflops_e2e:.1f}[/bold green]")

    print("-" * 60) # åˆ†å‰²çº¿

if __name__ == "__main__":
    args = parse_arguments()
    run(args.thiek, args.dtype, args.skip_test, args.add_bias, args.activation)
    print("TEST FINISHED (PASS)")